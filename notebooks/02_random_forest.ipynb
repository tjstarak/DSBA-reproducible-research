{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classification\n",
    "\n",
    "In this notebook we focus on reproducing the results of heart failure prediction with Random Forest. We will use the same model evaluation metrics as in the original paper, as well as same feature importance assessment methods and compare the results.\n",
    "\n",
    "The procedure is as follows:\n",
    "1. Split the data into train and test sets\n",
    "2. Fit the model on the train set \n",
    "3. Calculate evaluation metrics\n",
    "4. Calculate feature importance\n",
    "5. Repeat the procedure 100 times and aggregate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .csv file\n",
    "data = pd.read_csv('../data/heart_failure_records.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four key issues we faced when reproducing the results of the original paper:\n",
    "1. Lack of information on hyperparameters\n",
    "2. Inconsistent train/test splitting during model fitting and feature importance evaluation\n",
    "3. Calculating feature importance on train set\n",
    "4. Reporting best result out of 100 for some of the evaluation metrics instead of mean\n",
    "\n",
    "These will be explained in detail in the next sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "There is no information on hyperparameter tuning of Random Forest classifier. The authors are aware of the concept, since they apply tuning to SVM and Multi-layer Perceptron, but mention Random Forest with other methods that do not require tuning, such as Logistic Regression. This implies that authors used default hyperparameters specified in the `randomForest` R package.\n",
    "\n",
    "Based on this we can deduce that the hyperparameters are:\n",
    "- Number of trees: 500\n",
    "- Fraction of features sampled at each split: $\\sqrt n$, where $n$ is the number of features\n",
    "- Fraction of observation sampled at each split: 1 \n",
    "- Maximum leaf nodes in each tree: unlimited\n",
    "- Minimum size of terminal nodes / minimum samples in a leaf: 1\n",
    "\n",
    "It should be noted that while the default hyperparameter for `scikit-learn` implementation of Random Forest classifier are mostly the same, the most important hyperparameter, the number of trees grown, is different: 100. This can be only deduced from the code made available by the researchers and is not stated in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_scores = []\n",
    "pr_auc_scores = []\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "tp_scores = []\n",
    "tn_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Instantiate series for accumulating feature importance rankings\n",
    "rf_mdi_importance = pd.Series(0, index=data.drop(columns=['DEATH_EVENT', 'time']).columns, name='mean_impurity_decrease_ranksum')\n",
    "rf_perm_importance = pd.Series(0, index=data.drop(columns=['DEATH_EVENT', 'time']).columns, name='mean_accuracy_decrease_ranksum')\n",
    "\n",
    "# We do not need to use the same seed as researchers, since random number generator implementations\n",
    "# are different in R and NumPy, so the partitions will always be different.\n",
    "# We set this only once, since we want different partitions in each run.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Both performance assessment and feature importance are averaged over 100 runs\n",
    "for i in range(100):\n",
    "    # Partition data into 80/20 training/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['DEATH_EVENT', 'time']), data['DEATH_EVENT'], test_size=0.2)\n",
    "\n",
    "    # Instantiate, train, predict\n",
    "    rf = RandomForestClassifier(n_estimators=500)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Calculate performance assessment metrics\n",
    "    roc_auc_scores.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    y, x, _ = metrics.precision_recall_curve(y_test, y_pred)\n",
    "    pr_auc_scores.append(metrics.auc(x, y))\n",
    "    accuracy_scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(metrics.f1_score(y_test, y_pred))\n",
    "    tp_scores.append(metrics.recall_score(y_test, y_pred))\n",
    "    tn_scores.append(metrics.recall_score(y_test, y_pred, pos_label=0))\n",
    "    mcc_scores.append(metrics.matthews_corrcoef(y_test, y_pred))\n",
    "\n",
    "    # Partition data into 70/30 training/test - researchers used different split than in performance assessment\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['DEATH_EVENT', 'time']), data['DEATH_EVENT'], test_size=0.3)\n",
    "\n",
    "    # Instantiate, train, predict using new partitions\n",
    "    rf = RandomForestClassifier(n_estimators=500)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate and rank feature importances\n",
    "    rf_importance_1 = pd.Series(\n",
    "        rf.feature_importances_,\n",
    "        index=rf.feature_names_in_,\n",
    "        name='mean_impurity_decrease'\n",
    "    ).sort_values().rank(ascending=False)\n",
    "\n",
    "    # Calculate permutation importance on training data, as in the paper\n",
    "    result = permutation_importance(rf, X_train, y_train, n_repeats=5)\n",
    "    rf_importance_2 = pd.Series(\n",
    "        result['importances_mean'],\n",
    "        index=rf.feature_names_in_,\n",
    "        name='mean_accuracy_decrease'\n",
    "    ).sort_values().rank(ascending=False)\n",
    "\n",
    "    # Accumulate rankings\n",
    "    rf_mdi_importance += rf_importance_1\n",
    "    rf_perm_importance += rf_importance_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MCC score</th>\n",
       "      <td>0.629754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.526710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP rate</th>\n",
       "      <td>0.467010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN rate</th>\n",
       "      <td>0.866360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR AUC</th>\n",
       "      <td>0.635579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Random Forest\n",
       "MCC score       0.629754\n",
       "F1 score        0.526710\n",
       "Accuracy        0.850000\n",
       "TP rate         0.467010\n",
       "TN rate         0.866360\n",
       "PR AUC          0.635579\n",
       "ROC AUC         0.787500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = {\n",
    "    'MCC score': np.max(mcc_scores),\n",
    "    'F1 score': np.mean(f1_scores),\n",
    "    'Accuracy': np.max(accuracy_scores),\n",
    "    'TP rate': np.mean(tp_scores),\n",
    "    'TN rate': np.mean(tn_scores),\n",
    "    'PR AUC': np.mean(pr_auc_scores),\n",
    "    'ROC AUC': np.max(roc_auc_scores),\n",
    "}\n",
    "pd.DataFrame.from_dict(evaluations, orient='index', columns=['Random Forest'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above reproduces the Random Forest evaluation metrics from Table 4 in the original metrics. It should be noted that as in the original paper, ROC AUC, Accuracy and MCC are based on the best performing fits from the 100 repetitions, while the rest of the metrics are based on the means. It is not clear why the researchers chose to present the results this way, but given the inherent variability due to random train/test splitting, we consider this a mistake. This also makes it very difficult to reproduce the results. The impact is best seen on MCC score, where we achieve 0.63 score (higher is better), whereas the original paper reported 0.384. Similarily we achieved 0.85 accuracy on best fit while the researchers reported 0.74. The metrics using mean values are much more consistent. While still not exactly the same as in the original paper, this is expected due to different train/test splits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance assessment\n",
    "\n",
    "Researchers calculated all feature importance rankings on training data. We consider this a methodological mistake - with deep trees grown on such a small number of observations (`n=299`), the results are of little value, since they are likely driven by noise in the data, rather than underlying relationships. The goal of the paper is to use feature importance for selecting the top features, to be then used to make predictions on the test set with a more parsimonious model. With that goal in mind, it would be preferable in our view to split the data into training, validation and test set and calculate relevant importances on the validation set (e.g. permutation importance). Also, the authors state that the training set is 70% of all observations, which is incosistent to 80% stated in model performance assessment. \n",
    "\n",
    "Due to differences in random number generation, it is impossible to fully reproduce the figures in another language, however the researchers repeated importance calculation on 100 different splits, which should minimize the differences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rank the features by their importance: the higher the decrease in either impurity or accuracy, the lower the rank - i.e. the more important the feature is.\n",
    "\n",
    "The ranks from individual runs are aggregated using using Borda's method - i.e. sum individual ranks from multiple runs and rank the sums in ascending order. The same method is applied to aggregating the results of two feature importance assessment methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_mdi_dec</th>\n",
       "      <th>rank_acc_dec</th>\n",
       "      <th>rank_sum</th>\n",
       "      <th>rank_borda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ejection_fraction</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serum_creatinine</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platelets</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serum_sodium</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <td>7.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anaemia</th>\n",
       "      <td>7.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking</th>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          rank_mdi_dec  rank_acc_dec  rank_sum  rank_borda\n",
       "ejection_fraction                  2.0           1.0       3.0         1.0\n",
       "serum_creatinine                   1.0           2.0       3.0         1.0\n",
       "age                                3.0           3.0       6.0         3.0\n",
       "platelets                          4.0           4.0       8.0         4.0\n",
       "creatinine_phosphokinase           5.0           5.0      10.0         5.0\n",
       "serum_sodium                       6.0           6.0      12.0         6.0\n",
       "high_blood_pressure                7.5           7.0      14.5         7.0\n",
       "anaemia                            7.5           8.0      15.5         8.0\n",
       "diabetes                          10.0           9.0      19.0         9.0\n",
       "sex                                9.0          11.0      20.0        10.0\n",
       "smoking                           11.0          10.0      21.0        11.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame(rf_mdi_importance).join(rf_perm_importance)\n",
    "\n",
    "importances['rank_mdi_dec'] = importances['mean_impurity_decrease_ranksum'].rank()\n",
    "importances['rank_acc_dec'] = importances['mean_accuracy_decrease_ranksum'].rank()\n",
    "importances['rank_sum'] = importances['rank_mdi_dec'] + importances['rank_acc_dec']\n",
    "importances['rank_borda'] = importances['rank_sum'].rank(method='min')\n",
    "importances = importances.drop(columns=['mean_impurity_decrease_ranksum', 'mean_accuracy_decrease_ranksum'])\n",
    "\n",
    "importances.sort_values('rank_borda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above reproduces key results presented in Table 8 in the original paper. \n",
    "\n",
    "The results are broadly aligned - top three features are the same. However, in the original paper authros found the Serum creatinine feature to be the most important, with both methods ranking it first. In our results, the two methods disagree, hence both features are considered equally important. We suspect this is due to the fact that the authors used training data to calculate feature importance, which is prone to overfitting and more likely to be driven by random noise.\n",
    "\n",
    "Outside of top three features, the results diverge, e.g. we found Platelets feature to be the 4th most important, while the authors found it to be the 6th most important, we also found Age to be the 2nd least important feature, while authors placed it in the middle of the pack.\n",
    "\n",
    "Our results confirm that the authors' choice of the top two features for the final model was justified. However, the misalignment between the rest of the ranking casts doubt on robustness of Random Forest and feature importance ranking as part of the scientific method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Overall, our key results align with authors' findings with Random Forest. Due to a number of decisions by the original paper's authors, it is impossible to fully reproduce the results. We believe that the authors should have provided more information on the hyperparameter tuning and should have used more robust methods for feature importance assessment. We also believe that the performance assessment was flawed, since authors decided to report best results instead of means."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
