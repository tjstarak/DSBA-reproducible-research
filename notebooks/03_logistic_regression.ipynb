{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification\n",
    "\n",
    "In this notebook we focus on reproducing the results of heart failure prediction with **Logistic Regression**. We will use the same model evaluation metrics as in the original paper and compare the results.\n",
    "\n",
    "In the paper, logistic regression is seemingly used for different purposes. \n",
    "We can see in [Table 4](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5/tables/4) that a linear model was used to compare it against other Machine Learning methods.<br>\n",
    "Also, in [Table 10](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5/tables/10) the full model including a temporal component (*follow-up time*) was used to determine the feature ranking.</br>\n",
    "Based on that the authors decided to compare a full model to a resticed model using only *ejection fraction*, *serum creatinine*, and *follow-up time* and  as shown in [Table 11](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5/tables/11).  \n",
    "\n",
    "In this reproduction we will focus on the comparison of the restricted and unrestricted model.\n",
    "\n",
    "The procedure is as follows:\n",
    "1. Transform follow-up time to month as a factor\n",
    "2. Split the data into train and test sets\n",
    "3. Fit the model on the train set \n",
    "4. Calculate evaluation metrics\n",
    "5. Calculate feature importance\n",
    "6. Repeat the procedure 100 times and aggregate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .csv file\n",
    "data = pd.read_csv('../data/heart_failure_records.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four key issues we faced when reproducing the results of the original paper:\n",
    "1. Use of Linear Regression instead of Logistic Regression for a binary classification model \n",
    "2. Inconsistent train/test splitting during model fitting and feature importance evaluation\n",
    "3. Calculating feature importance without train/test splitting\n",
    "4. Using the mean for the evaluation metrics but comparing them to the best results out of 100 for an alternative logistic regression\n",
    "\n",
    "These will be explained in detail in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duzzi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\duzzi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "roc_auc_scores = []\n",
    "pr_auc_scores = []\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "tp_scores = []\n",
    "tn_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Instantiate series for accumulating feature importance rankings\n",
    "lr_mdi_importance = pd.Series(0, index=data.drop(columns=['DEATH_EVENT', 'time']).columns, name='mean_impurity_decrease_ranksum')\n",
    "lr_perm_importance = pd.Series(0, index=data.drop(columns=['DEATH_EVENT', 'time']).columns, name='mean_accuracy_decrease_ranksum')\n",
    "\n",
    "# We do not need to use the same seed as researchers, since random number generator implementations\n",
    "# are different in R and NumPy, so the partitions will always be different.\n",
    "# We set this only once, since we want different partitions in each run.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Both performance assessment and feature importance are averaged over 100 runs\n",
    "for i in range(100):\n",
    "    # Partition data into 80/20 training/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['DEATH_EVENT', 'time']), data['DEATH_EVENT'], test_size=0.2)\n",
    "\n",
    "    # Instantiate, train, predict\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # Calculate performance assessment metrics\n",
    "    roc_auc_scores.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    y, x, _ = metrics.precision_recall_curve(y_test, y_pred)\n",
    "    pr_auc_scores.append(metrics.auc(x, y))\n",
    "    accuracy_scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(metrics.f1_score(y_test, y_pred))\n",
    "    tp_scores.append(metrics.recall_score(y_test, y_pred))\n",
    "    tn_scores.append(metrics.recall_score(y_test, y_pred, pos_label=0))\n",
    "    mcc_scores.append(metrics.matthews_corrcoef(y_test, y_pred))\n",
    "\n",
    "#     # Partition data into 70/30 training/test - researchers used different split than in performance assessment\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['DEATH_EVENT', 'time']), data['DEATH_EVENT'], test_size=0.3)\n",
    "\n",
    "#     # Instantiate, train, predict using new partitions\n",
    "#     lr = LogisticRegression()\n",
    "#     lr.fit(X_train, y_train)\n",
    "\n",
    "#     # Calculate and rank feature importances\n",
    "#     lr_importance_1 = pd.Series(\n",
    "#         lr.feature_importances_,\n",
    "#         index=lr.feature_names_in_,\n",
    "#         name='mean_impurity_decrease'\n",
    "#     ).sort_values().rank(ascending=False)\n",
    "\n",
    "#     # Calculate permutation importance on training data, as in the paper\n",
    "#     result = permutation_importance(lr, X_train, y_train, n_repeats=5)\n",
    "#     lr_importance_2 = pd.Series(\n",
    "#         result['importances_mean'],\n",
    "#         index=lr.feature_names_in_,\n",
    "#         name='mean_accuracy_decrease'\n",
    "#     ).sort_values().rank(ascending=False)\n",
    "\n",
    "#     # Accumulate rankings\n",
    "#     lr_mdi_importance += lr_importance_1\n",
    "#     lr_perm_importance += lr_importance_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MCC score</th>\n",
       "      <td>0.629754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 score</th>\n",
       "      <td>0.443796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP rate</th>\n",
       "      <td>0.350412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN rate</th>\n",
       "      <td>0.916060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR AUC</th>\n",
       "      <td>0.623690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.765873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Random Forest\n",
       "MCC score       0.629754\n",
       "F1 score        0.443796\n",
       "Accuracy        0.883333\n",
       "TP rate         0.350412\n",
       "TN rate         0.916060\n",
       "PR AUC          0.623690\n",
       "ROC AUC         0.765873"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = {\n",
    "    'MCC score': np.max(mcc_scores),\n",
    "    'F1 score': np.mean(f1_scores),\n",
    "    'Accuracy': np.max(accuracy_scores),\n",
    "    'TP rate': np.mean(tp_scores),\n",
    "    'TN rate': np.mean(tn_scores),\n",
    "    'PR AUC': np.mean(pr_auc_scores),\n",
    "    'ROC AUC': np.max(roc_auc_scores),\n",
    "}\n",
    "pd.DataFrame.from_dict(evaluations, orient='index', columns=['Logistic Regression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance assessment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rank the features by their importance: the higher the decrease in either impurity or accuracy, the lower the rank - i.e. the more important the feature is.\n",
    "\n",
    "The ranks from individual runs are aggregated using using Borda's method - i.e. sum individual ranks from multiple runs and rank the sums in ascending order. The same method is applied to aggregating the results of two feature importance assessment methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_mdi_dec</th>\n",
       "      <th>rank_acc_dec</th>\n",
       "      <th>rank_sum</th>\n",
       "      <th>rank_borda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ejection_fraction</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serum_creatinine</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platelets</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serum_sodium</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <td>7.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anaemia</th>\n",
       "      <td>7.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking</th>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          rank_mdi_dec  rank_acc_dec  rank_sum  rank_borda\n",
       "ejection_fraction                  2.0           1.0       3.0         1.0\n",
       "serum_creatinine                   1.0           2.0       3.0         1.0\n",
       "age                                3.0           3.0       6.0         3.0\n",
       "platelets                          4.0           4.0       8.0         4.0\n",
       "creatinine_phosphokinase           5.0           5.0      10.0         5.0\n",
       "serum_sodium                       6.0           6.0      12.0         6.0\n",
       "high_blood_pressure                7.5           7.0      14.5         7.0\n",
       "anaemia                            7.5           8.0      15.5         8.0\n",
       "diabetes                          10.0           9.0      19.0         9.0\n",
       "sex                                9.0          11.0      20.0        10.0\n",
       "smoking                           11.0          10.0      21.0        11.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame(lr_mdi_importance).join(lr_perm_importance)\n",
    "\n",
    "importances['rank_mdi_dec'] = importances['mean_impurity_decrease_ranksum'].rank()\n",
    "importances['rank_acc_dec'] = importances['mean_accuracy_decrease_ranksum'].rank()\n",
    "importances['rank_sum'] = importances['rank_mdi_dec'] + importances['rank_acc_dec']\n",
    "importances['rank_borda'] = importances['rank_sum'].rank(method='min')\n",
    "importances = importances.drop(columns=['mean_impurity_decrease_ranksum', 'mean_accuracy_decrease_ranksum'])\n",
    "\n",
    "importances.sort_values('rank_borda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
